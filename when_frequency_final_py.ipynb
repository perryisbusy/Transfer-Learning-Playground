{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40641dc0",
   "metadata": {},
   "source": [
    "# `WHEN Frequency for Activity AU Description`\n",
    "\n",
    "### `Created/Modified By: Z Liu`\n",
    " \n",
    "* `1, Rewrite architecture of the code`\n",
    "* `2, Fix a warning \"a value is trying to be set on a copy\"; using df.at[] or df.loc[] instead of df[][]`\n",
    "* `3, Modify code so as to capture adhoc & ad hoc` \n",
    "* `4, Add similarity attribute to compare predictions and ground truth`\n",
    "* `5, Modify code so as to capture WHEN containing conjunction such as annual and quarterly `\n",
    "* `6, Remove stop words on predictions and ground truth before comparing similarity`\n",
    "* `7, Modify code so as to capture WHEN containing ... as ... basis (monthly as needed basis)`\n",
    "* `8, Be able to capture bi-..., bi ..., bi... etc`\n",
    "* `9, Solve error, \"float is not subscriptable\", desc = str(desc)`\n",
    "* `10, Add confidence, starting index, ending index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70206ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcafixmlpython/.local/lib/python3.6/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_md' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "Some weights of the model checkpoint at /home/mcafixmlpython/lib/bert-base-nli-mean-tokens/ were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.87 s, sys: 2.13 s, total: 11 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Deep Learning Libraries\n",
    "import spacy\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering,pipeline\n",
    "\n",
    "import nltk\n",
    "# nltk.data.path.append('/home/mca_fix/share/nltk_data/') \n",
    "nltk.data.path.append('/home/mcafixmlpython/lib/nltk_data/') \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# nlp = spacy.load(\"/home/mca_fix/share/en_core_web_md-3.0.0/en_core_web_md/en_core_web_md-3.0.0\")\n",
    "nlp = spacy.load(\"/home/mcafixmlpython/lib/en_core_web_md-3.0.0/en_core_web_md/en_core_web_md-3.0.0\")\n",
    "\n",
    "# Loading BERT-BASE-NLI-MEAN-TOKENS for similarity\n",
    "\n",
    "# simi_PATH = '/home/mca_fix/share/bert-base-nli-mean-tokens/'\n",
    "simi_PATH = '/home/mcafixmlpython/lib/bert-base-nli-mean-tokens/'\n",
    "\n",
    "simi_tokenizer=AutoTokenizer.from_pretrained(simi_PATH)\n",
    "simi_model=AutoModel.from_pretrained(simi_PATH,local_files_only=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd35edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def similar(sent):\n",
    "    tokens={'input_ids':[],'attention_mask':[]}\n",
    "    for sentence in sent:\n",
    "        new_tokens=simi_tokenizer.encode_plus(sentence,max_length=128,truncation=True,padding='max_length',return_tensors='pt')\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    #reformat list of tensors into single tensor\n",
    "    tokens['input_ids']=torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask']=torch.stack(tokens['attention_mask'])\n",
    "    #processing tokens\n",
    "    outputs=simi_model(**tokens)\n",
    "    outputs.keys()\n",
    "    embeddings=outputs.last_hidden_state\n",
    "    attention_mask=tokens['attention_mask']\n",
    "    mask=attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings=embeddings * mask\n",
    "    summed = torch.sum(masked_embeddings,1)\n",
    "    summed_mask=torch.clamp(mask.sum(1),min=1e-9)\n",
    "    mean_pooled=summed/summed_mask\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    mean_pooled=mean_pooled.detach().numpy()\n",
    "    x=cosine_similarity([mean_pooled[0]],mean_pooled[1:])\n",
    "    return x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f868ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    #print(\"in remove_stopwords\\n\",input_text)\n",
    "       \n",
    "    stopwords_list = stopwords.words('english')\n",
    "    newStopWords = ['citi']\n",
    "    stopwords_list.extend(newStopWords)\n",
    "        \n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    #whitelist = [\"n't\", \"not\", \"no\"]\n",
    "      \n",
    "    whitelist = [\"n't\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 2]          \n",
    "    return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c9e22",
   "metadata": {},
   "source": [
    "### `20220113_ARCMs_sp.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f200a133",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df = pd.read_excel('20220113_ARCMs_sp.xlsx', sheet_name=\"AU-ARCM Details\", engine='openpyxl')\n",
    "\n",
    "# # df = df.applymap(str)\n",
    "\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f516629",
   "metadata": {},
   "source": [
    "### `1000 ARCMs.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a8cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "CPU times: user 2.61 s, sys: 53.9 ms, total: 2.66 s\n",
      "Wall time: 2.66 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity Instance Id (for AU)</th>\n",
       "      <th>Activity AU Description</th>\n",
       "      <th>Frequency of the activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>709382.0</td>\n",
       "      <td>Model Development Data Sourcing: On a daily (e...</td>\n",
       "      <td>daily basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>709382.0</td>\n",
       "      <td>Model Development Data Sourcing: On a daily (e...</td>\n",
       "      <td>daily basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>709382.0</td>\n",
       "      <td>Model Development Data Sourcing: On a daily (e...</td>\n",
       "      <td>daily basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>709382.0</td>\n",
       "      <td>Model Development Data Sourcing: On a daily (e...</td>\n",
       "      <td>daily basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709383.0</td>\n",
       "      <td>Model Development Data Sourcing confidential P...</td>\n",
       "      <td>daily basis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity Instance Id (for AU)  \\\n",
       "0                       709382.0   \n",
       "1                       709382.0   \n",
       "2                       709382.0   \n",
       "3                       709382.0   \n",
       "4                       709383.0   \n",
       "\n",
       "                             Activity AU Description Frequency of the activity  \n",
       "0  Model Development Data Sourcing: On a daily (e...               daily basis  \n",
       "1  Model Development Data Sourcing: On a daily (e...               daily basis  \n",
       "2  Model Development Data Sourcing: On a daily (e...               daily basis  \n",
       "3  Model Development Data Sourcing: On a daily (e...               daily basis  \n",
       "4  Model Development Data Sourcing confidential P...               daily basis  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df = pd.read_excel(io='1000 ARCMs.xlsx', sheet_name=\"1000 ARCMs\", engine='openpyxl')\n",
    "\n",
    "# df = df[df[\"Activity Description contains > 20 words\"] == \"Pass\"]\n",
    "df = df[[\"Activity Instance Id (for AU)\", \"Activity AU Description\", \"Frequency of the activity\"]]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# # coerce type conversion to every element of a DataFrame\n",
    "# df = df.applymap(str)\n",
    "\n",
    "# text preprocessing\n",
    "# df['Activity AU Description'] = df['Activity AU Description'].str.strip().str.lower()\n",
    "# df['Activity AU Description'] = df['Activity AU Description'].str.replace('[^\\w\\s]','') # semi-annual -> semiannual\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3807c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "when_frequency_lst = [\"daily\", \"weekly\", \n",
    "#                       \"biweekly\", \"realtime\", \"event driven\", # *** from control#\n",
    "                      \"monthly\", \"quarterly\", \"yearly\", \"semiannual\", \"semiannually\",\n",
    "                      \"annual\", # *** from activity# \"ongoing\", \"needed\", \n",
    "                      \"annually\", \n",
    "                      # outliers # \n",
    "                      \"each month\", \"every month\", \"calendar month\",  \n",
    "                      # \"needed basis\", \"required basis\",\n",
    "                      \"ad hoc\", \"adhoc\"]\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def activity_when(desc: str) -> Tuple[str, str, int, int]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Text preprocessing\n",
    "    desc = str(desc)\n",
    "#     desc = re.sub('[^\\w\\s]', '', desc.strip().lower())\n",
    "    desc = desc.lower()\n",
    "    doc = nlp(desc)\n",
    "    \n",
    "    activity_when_answer = None\n",
    "    \n",
    "    \n",
    "    ########################### semi/bi, semi-annually, bi-weekly... ###########################\n",
    "    semi_bi = [\"semi\", \"bi\"]\n",
    "    for word in when_frequency_lst:\n",
    "        \n",
    "        for semi_bi_word in semi_bi:\n",
    "            \n",
    "            if f\"\"\"{semi_bi_word}-{word}\"\"\" in desc:\n",
    "        \n",
    "                activity_when_answer = f\"\"\"{semi_bi_word}-{word}\"\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    ########################### annual and quarterly basis; adhoc or weekly basis... ########################### \n",
    "    for token in doc:\n",
    "        \n",
    "        # Exclude last three tokens \n",
    "        try:\n",
    "        \n",
    "            if (token.text in when_frequency_lst) and (doc[token.i+1].pos_ == \"CCONJ\") and (doc[token.i+2].text in when_frequency_lst) and (doc[token.i+3].text in [\"basis\", \"cadence\"]):\n",
    "                \n",
    "                span = \" \".join([doc[token.i].text, doc[token.i+1].text, doc[token.i+2].text, doc[token.i+3].text])\n",
    "\n",
    "                if any(word in when_frequency_lst for word in span.split()) and (activity_when_answer is None):\n",
    "  \n",
    "                    activity_when_answer = span\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### on ... basis... ########################### \n",
    "    for token in doc:\n",
    "        \n",
    "        # Exclude the last token \n",
    "        try:\n",
    "            \n",
    "            if (token.text in when_frequency_lst) and (doc[token.i+1].text in [\"basis\", \"cadence\"]):\n",
    "                \n",
    "                # ... as ... basis (monthly as needed basis)\n",
    "                if doc[token.i-2].text in when_frequency_lst:\n",
    "                    \n",
    "                    span = \" \".join([doc[token.i-2].text, doc[token.i-1].text, token.text, doc[token.i+1].text])\n",
    "                \n",
    "                # on a/an ... basis ..\n",
    "                elif (doc[token.i-2].text == \"on\") and ((doc[token.i-1].text == \"a\") or (doc[token.i-1].text == \"an\")):\n",
    "                    \n",
    "                    span = \" \".join([doc[token.i-2].text, doc[token.i-1].text, token.text, doc[token.i+1].text])\n",
    "                \n",
    "                # on ... basis ...\n",
    "                elif doc[token.i-1].text == \"on\":\n",
    "                    \n",
    "                    span = \" \".join([doc[token.i-1].text, token.text, doc[token.i+1].text])\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    span = \" \".join([token.text, doc[token.i+1].text])\n",
    "\n",
    "                if any(word in when_frequency_lst for word in span.split()) and (activity_when_answer is None):\n",
    "\n",
    "                        activity_when_answer = span\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "            \n",
    "              \n",
    "    ########################### monthly, quarterly, annually, semi-annually... ###########################       \n",
    "    for token in doc:\n",
    "        \n",
    "        # Exclude the last token \n",
    "        try:\n",
    "        \n",
    "            if ((token.dep_ == \"amod\") and (token.pos_ == \"ADJ\")) or ((token.dep_ == \"advmod\") and (token.pos_ == \"ADV\")):\n",
    "                \n",
    "                span = token.text\n",
    "\n",
    "                if any(word in when_frequency_lst for word in span.split()) and (activity_when_answer is None):\n",
    "\n",
    "                    activity_when_answer = span\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "                \n",
    "                \n",
    "                \n",
    "    ########################### other outliers... ###########################\n",
    "    if activity_when_answer is None:\n",
    "    \n",
    "        for word in when_frequency_lst:\n",
    "            \n",
    "            if word in desc:\n",
    "                \n",
    "                activity_when_answer = word\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### confidence ###########################\n",
    "    if activity_when_answer is None:\n",
    "        \n",
    "        confidence = \"Low\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        confidence = \"High\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    ########################### starting index, ending index ###########################\n",
    "    if activity_when_answer is None:\n",
    "        \n",
    "        start, end = None, None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        start = desc.find(activity_when_answer)\n",
    "        \n",
    "        if start == -1:\n",
    "            \n",
    "            start, end = None, None\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            end = start + len(activity_when_answer)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return activity_when_answer, confidence, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b8780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 46s, sys: 1min 40s, total: 28min 26s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# df[\"when_ans_freq\"] = df[\"Activity AU Description\"].apply(func=activity_when)\n",
    "df[[\"when_ans_freq\", \"confidence\", \"start\", \"end\"]] = df[\"Activity AU Description\"].\\\n",
    "    apply(func=lambda row: activity_when(row)[0:4]).to_list()\n",
    "df[\"when_ans_freq\"] = df[\"when_ans_freq\"].apply(func=lambda row : \"missing\" if row is None else row)\n",
    "df[\"found\"] = (~df[\"when_ans_freq\"].isna())\n",
    "df['review_priority'] = np.where((df['found'] == False), \"High Priority Review because of Missing When Frequency\", \"No Need to Review\")      \n",
    "df['missing_or_not'] = np.where((df['found'] == False), \"Missing\", \"Not Missing\")\n",
    "df[\"Frequency of the activity\"] = df[\"Frequency of the activity\"].str.strip().str.lower()\n",
    "df[\"when_ans_freq\"] = df[\"when_ans_freq\"].str.strip().str.lower()\n",
    "df[\"similarity\"] = df.apply(func=lambda row: similar(sent=[row[\"Frequency of the activity\"], row[\"when_ans_freq\"]]), \n",
    "                            axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
